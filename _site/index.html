<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Ankur Handa</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Ankur Handa" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link rel="canonical" href="http://localhost:5000/">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Ankur Handa
              </h1>
              <p style="text-align:center">
                <a target="_blank" href="https://mailhide.io/e/B8TyS"> Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/leonidk">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=GCYG4GoAAAAJ">Google Scholar</a> &nbsp;
              </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                I'm interested in computer vision, machine learning, optimization, graphics and robotics.
              </p>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/dexpilot.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>DexPilot: Vision Based Teleoperation of Dexterous Robotic Hand-Arm System</h3>
              <br>
              <strong>Ankur Handa*</strong>, Karl Van Wyk<strong>*</strong>, Wei Yang, Jacky Liang, Yu-Wei Chao, Qian Wan, Stan Birchfield, Nathan Ratliff, Dieter Fox (<strong>*</strong> Equal Contribution)

              <br>
              <em>arXiv preprint</em>, 2019
              <br>
              
              <a href="https://arxiv.org/abs/1910.03135">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>An entirely vision based teleoperation for robotic hand-arm system to collect data for imitation learning. The system works without any markers or gloves on the hand.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/contactgrasp.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>ContactGrasp: Functional Multi-finger Grasp Synthesis from Contact</h3>
              <br>
              Samarth Brahmbhatt, <strong>Ankur Handa</strong>, James Hays and Dieter Fox

              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2019
              <br>
              
              <a href="https://arxiv.org/abs/1904.03754">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We present ContactGrasp, a framework for functional grasp synthesis from object shape and contact on the object surface. Contact can be manually specified or obtained through demonstrations. Our contact representation is object-centric and allows functional grasp synthesis even for hand models different than the one used for demonstration.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/closing_sim_to_real_loop.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</h3>
              <br>
              Yevgen Chebotar, <strong>Ankur Handa</strong>, Viktor Makoviychuk, Miles Macklin, Jan Isaac, Nathan Ratliff, Dieter Fox

              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2019
              <br>
              
              <a href="https://arxiv.org/abs/1810.05687">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>The system closes the loop between simulations and real-world by training first in simulations and then testing it out in real-world. The real world data is then used to adjust the variances of the parameters that are randomised while training a policy with PPO.</p>
<p><font color="red"> Best Student Paper Award Finalist. </font></p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/tactile_force.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Robust Learning of Tactile Force Estimation through Robot Interaction</h3>
              <br>
              Balakumar Sundaralingam, Alexander (Sasha) Lambert, <strong>Ankur Handa</strong>, Byron Boots, Tucker Hermans, Stan Birchfield, Nathan Ratliff, Dieter Fox

              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2019
              <br>
              
              <a href="https://arxiv.org/abs/1810.06187">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>In this paper, we explore learning a robust model that maps tactile sensor signals to force. We specifically explore learning a mapping for the SynTouch BioTac sensor via neural networks.</p>
<p><font color="red"> Best Manipulation Paper Award Finalist. </font></p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/tactile_servoing.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Learning Latent Space Dynamics for Tactile Servoing</h3>
              <br>
              Giovanni Sutanto, Nathan Ratliff, Balakumar Sundaralingam, Yevgen Chebotar, Zhe Su, <strong>Ankur Handa</strong>, Dieter Fox

              <br>
              <em>International Confernece on Robotics and Automation (ICRA)</em>, 2019
              <br>
              
              <a href="https://arxiv.org/abs/1811.03704">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>In this paper, we specifically address the challenge of tactile servoing, <em>i.e.</em> given the current tactile sensing and a target/goal tactile sensing – memorized from a successful task execution in the past – what is the action that will bring the current tactile sensing to move closer towards the target tactile sensing at the next time step.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/gpu_sim.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning</h3>
              <br>
              <strong>Ankur Handa*</strong>, Jacky Liang<strong>*</strong>, Viktor Makoviychuk<strong>*</strong>, Nuttapong Chentanez, Miles Macklin, Dieter Fox  (<strong>*</strong>Equal Contribution)

              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2018
              <br>
              
              <a href="https://arxiv.org/abs/1810.05762">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>In this work, we propose using GPU-accelerated RL simulations as an alternative to CPU ones. Using NVIDIA Flex, a GPU-based physics engine, we show promising speed-ups of learning various continuous-control, locomotion tasks. With one GPU and CPU core, we are able to train the Humanoid running task with PPO in less than 20 minutes, using 10-1000x fewer CPU cores than previous works.</p>


            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/domain_random_grasps.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Domain randomization and generative models for robotic grasping</h3>
              <br>
              Josh Tobin, Lukas Biewald, Rocky Duan, Marcin Andrychowicz, <strong>Ankur Handa</strong>, Vikash Kumar, Bob McGrew, Alex Ray, Jonas Schneider, Peter Welinder, Wojciech Zaremba, Pieter Abbeel

              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2018
              <br>
              
              <a href="https://arxiv.org/abs/1710.06425">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>In this work, we explore a novel data generation pipeline for training a deep neural network to perform grasp planning that applies the idea of domain randomization to object synthesis. We generate millions of unique, unrealistic procedurally generated objects, and train a deep neural network to perform grasp planning on these objects.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/scenenet_rgbd_exps.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?</h3>
              <br>
              John McCormac, <strong>Ankur Handa</strong>, Stefan Leutenegger, Andrew J. Davison

              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2017
              <br>
              
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/McCormac_SceneNet_RGB-D_Can_ICCV_2017_paper.pdf">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We propose a new dataset of 5M indoor images with photorealistic rendering and ground truth for segmentation, depth and optical flow. By training on this huge dataset we are able to show that we can beat a network pre-trained on ImageNet on indoor scene segmentation.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/semantic_fusion.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks</h3>
              <br>
              John McCormac, <strong>Ankur Handa</strong>, Stefan Leutenegger, Andrew J. Davison

              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2017
              <br>
              
              <a href="https://arxiv.org/abs/1609.05130">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We combine Convolutional Neural Networks (CNNs) and a state of the art dense Simultaneous Localisation and Mapping(SLAM) system, ElasticFusion, which provides long-term dense correspondence between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN’s semantic predictions from multiple view points to be probabilistically fused into a map. Our system is efficient enough to allow real-time interactive use at frame-rates of approximately 25Hz.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/SceneNetRGBD.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth</h3>
              <br>
              John McCormac, <strong>Ankur Handa</strong>, Stefan Leutenegger, Andrew J. Davison

              <br>
              <em>arxiv</em>, 2016
              <br>
              
              <a href="https://arxiv.org/abs/1612.05079">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>An entirely vision based teleoperation for robotic hand-arm system to collect data for imitation learning. The system works without any markers or gloves on the hand.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/gvnn.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>gvnn: Neural Network Library for Geometric Computer Vision</h3>
              <br>
              <strong>Ankur Handa</strong>, Michael Bloesch, Viorica Patraucean, Simon Stent, John McCormac, Andrew Davison

              <br>
              <em>European Conference on Computer Vision (Workshop): Geometry meets Deep Learning</em>, 2016
              <br>
              
              <a href="https://arxiv.org/abs/1607.07405">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We introduce gvnn, a neural network library in Torch aimed towards bridging the gap between classic geometric computer vision and deep learning. Inspired by the recent success of Spatial Transformer Networks, we propose several new layers which are often used as parametric transformations on the data in geometric computer vision.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/SceneNet.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Understanding Real World Indoor Scenes With Synthetic Data</h3>
              <br>
              <strong>Ankur Handa</strong>, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla

              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016
              <br>
              
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Handa_Understanding_Real_World_CVPR_2016_paper.pdf">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/scenenet-gen.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>SceneNet: an Annotated Model Generator for Indoor Scene Understanding</h3>
              <br>
              <strong>Ankur Handa</strong>, Viorica Patraucean, Simon Stent, Roberto Cipolla

              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2016
              <br>
              
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7487797">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We introduce SceneNet, a framework for generating high-quality annotated 3D scenes to aid indoor scene understanding. SceneNet leverages manually-annotated datasets of real world scenes such as NYUv2 to learn statistics about object co-occurrences and their spatial relationships.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/diff-mem.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Spatio-Temporal Video Autoencoder With Differentiable Memory</h3>
              <br>
              Viorica Patraucean, <strong>Ankur Handa</strong>, Roberto Cipolla

              <br>
              <em>International Conference on Learning and Representations Workshop Track (ICLRW)</em>, 2016
              <br>
              
              <a href="https://arxiv.org/abs/1511.06309">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/hdr-fusion.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>HDRFusion: HDR SLAM using a low-cost auto-exposure RGB-D sensor</h3>
              <br>
              Shuda Li, <strong>Ankur Handa</strong>, Yang Zhang, Andrew Calway

              <br>
              <em>International Conference on 3D Vision (3DV)</em>, 2016
              <br>
              
              <a href="https://arxiv.org/abs/1604.00895">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We describe a new method for comparing frame appearance in a frame-to-model 3-D mapping and tracking system using an low dynamic range (LDR) RGB-D camera which is robust to brightness changes caused by auto exposure.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/segnet.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling</h3>
              <br>
              Vijay Badrinarayanan, <strong>Ankur Handa</strong>, Roberto Cipolla

              <br>
              <em>arXiv</em>, 2015
              <br>
              
              <a href="https://arxiv.org/abs/1505.07293">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We propose a novel deep architecture, SegNet, for semantic pixel wise image labelling. SegNet is composed of a stack of encoders followed by a corresponding decoder stack which feeds into a soft-max classification layer. The decoders help map low resolution feature maps at the output of the encoder stack to full input image size feature maps. This addresses an important drawback of recent deep learning approaches which have adopted networks designed for object categorization for pixel wise labelling.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/icl-nuim.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>A Benchmark for RGB-D Visual Odometry, 3D Reconstruction and SLAM</h3>
              <br>
              <strong>Ankur Handa</strong>, Thomas Whelan, John McDonald and Andrew J. Davison

              <br>
              <em>Interntaional Conference on Robotics and Automation (ICRA)</em>, 2014
              <br>
              
              <a href="http://mural.maynoothuniversity.ie/8309/1/JM-Benchmark-2014.pdf">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We introduce the Imperial College London and National University of Ireland Maynooth (ICL-NUIM) dataset for the evaluation of visual odometry, 3D reconstruction and SLAM algorithms that typically use RGB-D data. We present a collection of handheld RGB-D camera sequences within synthetically generated environments. RGB-D sequences with perfect ground truth poses are provided as well as a ground truth surface model that enables a method of quantitatively
evaluating the final map or surface reconstruction accuracy.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/event-cam.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Simultaneous Mosaicing and Tracking with an Event Camera</h3>
              <br>
              Hanme Kim, <strong>Ankur Handa</strong>, Ryad Benosman, Sio-Hoi Ieng, Andrew J. Davison

              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2014
              <br>
              
              <a href="http://www.bmva.org/bmvc/2014/files/paper066.pdf">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We show for the first time that an event stream, with no additional sensing, can be used to track accurate camera rotation while building a persistent and high quality mosaic of a scene which is super-resolution accurate and has high dynamic range.</p>
<p><font color="red"> Best Industry Paper Award Winner. </font></p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/vafric.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Real-Time Camera Tracking: When is High Frame-Rate Best?</h3>
              <br>
              <strong>Ankur Handa</strong>, Richard A. Newcombe, Adrien Angeli and Andrew J. Davison

              <br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2012
              <br>
              
              <a href="https://www.doc.ic.ac.uk/~ahanda/VaFRIC/eccv2012.pdf">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>Using 3D camera tracking as our test problem, and analysing a fundamental dense whole image alignment approach, we open up a route to a systematic investigation via the careful synthesis of photorealistic video using ray-tracing of a detailed 3D scene, experimentally obtained photometric response and
noise models, and rapid camera motions.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/lfreport.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Applications of Legendre-Fenchel transformation to computer vision problems</h3>
              <br>
              <strong>Ankur Handa</strong>, Richard A. Newcombe, Adrien Angeli and Andrew J. Davison

              <br>
              <em>Departmental Technical Report</em>, 2011
              <br>
              
              <a href="https://www.doc.ic.ac.uk/~ahanda/VaFRIC/eccv2012.pdf">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>This report presents a tutorial like walk through of the famous Pock and Chambolle primal-dual algorithm and provides a background on the fundametal legendre-fenchel transform that is a building block of the algorithm.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/sam.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Scalable Active Matching</h3>
              <br>
              <strong>Ankur Handa</strong>, Margarita Chli, Hauke Strasdat and Andrew J. Davison

              <br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2010
              <br>
              
              <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.948.2385&rep=rep1&type=pdf">arxiv</a> /
              
              
              
              
              
              <p></p>
              <p>We propose two variants called SubAM and CLAM in the context of camera tracking by sequential feature matching with rigorous decisions guided by Information Theory. These variants allow us to match significantly more features than the original AM (Active Matching) algorithm.</p>

            </td>
          </tr>
          
          
          
        </table>
        <br>
        <br>
        <br>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

